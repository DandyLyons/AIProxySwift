//
//  OpenAIChatCodables.swift
//
//
//  Created by Lou Zell on 6/11/24.
//

import Foundation


// MARK: - Request Codables

/// Chat completion request body. See the OpenAI reference for available fields.
/// Contributions are welcome if you need something beyond the simple fields I've added so far.
/// Docstrings are taken from this reference:
/// https://platform.openai.com/docs/api-reference/chat/create
public struct OpenAIChatCompletionRequestBody: Encodable {

    // Required
    public let model: String
    public let messages: [OpenAIChatMessage]

    // Optional
    public let responseFormat: OpenAIChatResponseFormat?
    public var stream: Bool?
    public var streamOptions: OpenAIChatStreamOptions?

    /// What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the
    /// output more random, while lower values like 0.2 will make it more focused and
    /// deterministic.
    ///
    /// We generally recommend altering this or `top_p` but not both.
    ///
    /// If not set, OpenAI defaults this value to 1.
    public let temperature: Double?

    /// An alternative to sampling with `temperature`, called nucleus sampling, where the model
    /// considers the results of the tokens with `top_p` probability mass. So 0.1 means only the
    /// tokens comprising the top 10% probability mass are considered.
    ///
    /// We generally recommend altering this or `temperature` but not both.
    /// If not set, OpenAI defaults this value to 1
    public let topP: Double?

    private enum CodingKeys: String, CodingKey {
        case messages
        case model
        case responseFormat = "response_format"
        case stream
        case streamOptions = "stream_options"
        case temperature
        case topP = "top_p"
    }

    // This memberwise initializer is autogenerated.
    // To regenerate, use `cmd-shift-a` > Generate Memberwise Initializer
    // To format, place the cursor in the initializer's parameter list and use `ctrl-m`
    public init(
        model: String,
        messages: [OpenAIChatMessage],
        responseFormat: OpenAIChatResponseFormat? = nil,
        stream: Bool? = nil,
        streamOptions: OpenAIChatStreamOptions? = nil,
        temperature: Double? = nil,
        topP: Double? = nil
    ) {
        self.model = model
        self.messages = messages
        self.responseFormat = responseFormat
        self.stream = stream
        self.streamOptions = streamOptions
        self.temperature = temperature
        self.topP = topP
    }
}

public struct OpenAIChatMessage: Encodable {
    public let role: String
    public let content: OpenAIChatContent

    public init(role: String, content: OpenAIChatContent) {
        self.role = role
        self.content = content
    }
}

public enum OpenAIChatResponseFormat: Encodable {
    case type(String)

    enum CodingKeys: String, CodingKey {
        case type
    }

    public func encode(to encoder: Encoder) throws {
        var container = encoder.container(keyedBy: CodingKeys.self)
        switch self {
        case .type(let format):
            try container.encode(format, forKey: .type)
        }
    }
}

/// ChatContent is made up of either a single piece of text or a collection of ChatContentParts
public enum OpenAIChatContent: Encodable {
    case text(String)
    case parts([OpenAIChatContentPart])

    public func encode(to encoder: Encoder) throws {
       var container = encoder.singleValueContainer()
       switch self {
       case .text(let text):
          try container.encode(text)
       case .parts(let contentParts):
          try container.encode(contentParts)
       }
    }
}

public enum OpenAIChatContentPart: Encodable {

    case text(String)
    case imageURL(URL)

    enum CodingKeys: String, CodingKey {
        case type
        case text
        case imageURL = "image_url"
    }

    public func encode(to encoder: Encoder) throws {
        var container = encoder.container(keyedBy: CodingKeys.self)
        switch self {
        case .text(let text):
            try container.encode("text", forKey: .type)
            try container.encode(text, forKey: .text)
        case .imageURL(let url):
            try container.encode("image_url", forKey: .type)
            try container.encode(OpenAIImageURL(url: url), forKey: .imageURL)
        }
    }
}

public struct OpenAIChatStreamOptions: Encodable {
   /// If set, an additional chunk will be streamed before the data: [DONE] message.
   /// The usage field on this chunk shows the token usage statistics for the entire request,
   /// and the choices field will always be an empty array. All other chunks will also include
   /// a usage field, but with a null value.
   let includeUsage: Bool

   enum CodingKeys: String, CodingKey {
       case includeUsage = "include_usage"
   }
}


private struct OpenAIImageURL: Encodable {
    let url: URL
}


// MARK: - Response Codables
// MARK: Non-streaming
public struct OpenAIChatCompletionResponseBody: Decodable {
    public let model: String
    public let choices: [OpenAIChatChoice]
}

public struct OpenAIChatChoice: Decodable {
    public let message: OpenAIChoiceMessage
    public let finishReason: String

    enum CodingKeys: String, CodingKey {
        case message
        case finishReason = "finish_reason"
    }
}

public struct OpenAIChoiceMessage: Decodable {
    public let role: String
    public let content: String
}


// MARK: Streaming
public struct OpenAIChatCompletionChunk: Codable {
    public let choices: [OpenAIChunkChoice]
}

public struct OpenAIChunkChoice: Codable {
    public let delta: OpenAIChunkDelta
    public let finishReason: String?

    enum CodingKeys: String, CodingKey {
        case delta
        case finishReason = "finish_reason"
    }
}

public struct OpenAIChunkDelta: Codable {
    public let role: String?
    public let content: String?
}


// MARK: - Internal extensions
extension OpenAIChatCompletionChunk {
    /// Creates a ChatCompletionChunk from a streamed line of the /v1/chat/completions response
    static func from(line: String) -> Self? {
        guard line.hasPrefix("data: ") else {
            aiproxyLogger.warning("Received unexpected line from aiproxy: \(line)")
            return nil
        }

        guard line != "data: [DONE]" else {
            aiproxyLogger.debug("Streaming response has finished")
            return nil
        }

        guard let chunkJSON = line.dropFirst(6).data(using: .utf8),
              let chunk = try? JSONDecoder().decode(OpenAIChatCompletionChunk.self, from: chunkJSON) else
        {
            aiproxyLogger.warning("Received unexpected JSON from aiproxy: \(line)")
            return nil
        }

        // aiproxyLogger.debug("Received a chunk: \(line)")
        return chunk
    }
}
